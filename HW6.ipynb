{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Exercise from http://www.nltk.org/book_1ed/ch07.html\n",
      "###Author : Nirmal kumar Ravi"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> The IOB format categorizes tagged tokens as I, O and B. Why are three tags necessary? What problem would be caused if we used I and O tags exclusively?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* IOB stands for (I)inside (O)outside (B)egining tags\n",
      "* B is esential because It marks the begining of token\n",
      "* If we don't have a 'B' Then It would not be possible to identify tokens If tokens appear next to each other.\n",
      "* In other words B indicates the begining of a new token"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Write a tag pattern to match noun phrases containing plural head nouns, e.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\". Try to do this by generalizing the tag pattern that handled singular noun phrases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "grammar = \"PHN: {<[CDJT].*>+<NNS>}\"\n",
      "cp = nltk.RegexpParser(grammar)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Pick one of the three chunk types in the CoNLL corpus. Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences that make up this kind of chunk. Develop a simple chunker using the regular expression chunker nltk.RegexpParser. Discuss any tag sequences that are difficult to chunk reliably."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import conll2000\n",
      "print conll2000.chunked_sents('train.txt', chunk_types=['NP'])[100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S (NP He/PRP) talked/VBD (NP about/IN 20/CD minutes/NNS) ./.)\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We have filtered the text with noun phrase\n",
      "* The Sentence is \"He talked about 20 minutes\"\n",
      "* Here there are two chunks \"HE\" and \"About 20 Minutes\"\n",
      "* POS taging are PRP, VBD, IN, CD, NNS "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
      "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "print cp.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ChunkParse score:\n",
        "    IOB Accuracy:  87.7%\n",
        "    Precision:     70.6%\n",
        "    Recall:        67.8%\n",
        "    F-Measure:     69.2%\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> An early definition of chunk was the material that occurs between chinks. Develop a chunker that starts by putting the whole sentence in a single chunk, and then does the rest of its work solely by chinking. Determine which tags (or tag sequences) are most likely to make up chinks with the help of your own utility program. Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "* In this we are asked to develop chinking instead of chunking.\n",
      "* For eg the sentence \"we saw the yellow dog\". Has two NP chunks \"we\",  \"the yellow dog\"\n",
      "* So in chinking we will leave the rest and try to chink \"Yellow\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"We\",\"PRP\"),(\"saw\",\"VBD\"),(\"the\",\"DT\"),(\"yellow\",\"JJ\"),(\"dog\",\"NN\")]\n",
      "grammar = \"CHINK: {<VBD>}\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "result = cp.parse(sentence)\n",
      "print result\n",
      "result.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S We/PRP (CHINK saw/VBD) the/DT yellow/JJ dog/NN)\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Write a tag pattern to cover noun phrases that contain gerunds, e.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\". Add these patterns to the grammar, one per line. Test your work using some tagged sentences of your own devising."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = \"GRD: {<DT>|<NN><VBJ><NN>}\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "brown = nltk.corpus.brown\n",
      "for sent in brown.tagged_sents():\n",
      "    result = cp.parse(sent)\n",
      "    print result\n",
      "    break #I dont want the entire brown corpous printed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  The/AT\n",
        "  Fulton/NP-TL\n",
        "  County/NN-TL\n",
        "  Grand/JJ-TL\n",
        "  Jury/NN-TL\n",
        "  said/VBD\n",
        "  Friday/NR\n",
        "  an/AT\n",
        "  investigation/NN\n",
        "  of/IN\n",
        "  Atlanta's/NP$\n",
        "  recent/JJ\n",
        "  primary/NN\n",
        "  election/NN\n",
        "  produced/VBD\n",
        "  ``/``\n",
        "  no/AT\n",
        "  evidence/NN\n",
        "  ''/''\n",
        "  that/CS\n",
        "  any/DTI\n",
        "  irregularities/NNS\n",
        "  took/VBD\n",
        "  place/NN\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Write one or more tag patterns to handle coordinated noun phrases, e.g. \"July/NNP and/CC August/NNP\", \"all/DT your/PRP$ managers/NNS and/CC supervisors/NNS\", \"company/NN courts/NNS and/CC adjudicators/NNS\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* It follows the pattern NN? followed by a conjuction \"and\" then NN?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"July\",\"NNP\"),(\"and\",\"CC\"),(\"August\",\"NNP\")]\n",
      "grammar = \"CNP: {<NN[P|S]><CC><NN[P|S]>}\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "brown = nltk.corpus.brown\n",
      "result = cp.parse(sentence)\n",
      "print result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S (CNP July/NNP and/CC August/NNP))\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Carry out the following evaluation tasks for any of the chunkers you have developed earlier. (Note that most chunking corpora contain some internal inconsistencies, such that any reasonable rule-based approach will produce errors.)\n",
      "Evaluate your chunker on 100 sentences from a chunked corpus, and report the precision, recall and F-measure.\n",
      "Use the chunkscore.missed() and chunkscore.incorrect() methods to identify the errors made by your chunker. Discuss.\n",
      "Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import conll2000\n",
      "from nltk.chunk import *\n",
      "from nltk.chunk.util import *\n",
      "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
      "print cp.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ChunkParse score:\n",
        "    IOB Accuracy:  87.7%\n",
        "    Precision:     70.6%\n",
        "    Recall:        67.8%\n",
        "    F-Measure:     69.2%\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#base line chunker\n",
      "from nltk.corpus import conll2000\n",
      "grammar = \"\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
      "print cp.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ChunkParse score:\n",
        "    IOB Accuracy:  43.4%\n",
        "    Precision:      0.0%\n",
        "    Recall:         0.0%\n",
        "    F-Measure:      0.0%\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* comparing to baseline chunker . Our chunker performs extremly well"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}