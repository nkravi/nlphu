{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Exercise from http://www.nltk.org/book_1ed/ch05.html\n",
      "###Author : Nirmal kumar Ravi"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Tokenize and tag the following sentence: They wind back the clock, while we chase after the wind. What different pronunciations and parts of speech are involved?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "text = nltk.word_tokenize(\"They wind back the clock, while we chase after the wind\")\n",
      "tagged = nltk.pos_tag(text)\n",
      "for i in range(0,len(tagged)):\n",
      "    txt,tag = tagged[i]\n",
      "    print txt,tag,nltk.help.upenn_tagset(tag)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "They PRPPRP: pronoun, personal\n",
        "    hers herself him himself hisself it itself me myself one oneself ours\n",
        "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
        " None\n",
        "wind VBPVBP: verb, present tense, not 3rd person singular\n",
        "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
        "    appear tend stray glisten obtain comprise detest tease attract\n",
        "    emphasize mold postpone sever return wag ...\n",
        " None\n",
        "back RBRB: adverb\n",
        "    occasionally unabatingly maddeningly adventurously professedly\n",
        "    stirringly prominently technologically magisterially predominately\n",
        "    swiftly fiscally pitilessly ...\n",
        " None\n",
        "the DTDT: determiner\n",
        "    all an another any both del each either every half la many much nary\n",
        "    neither no some such that the them these this those\n",
        " None\n",
        "clock NNNN: noun, common, singular or mass\n",
        "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
        "    investment slide humour falloff slick wind hyena override subhumanity\n",
        "    machinist ...\n",
        " None\n",
        ", ,,: comma\n",
        "    ,\n",
        " None\n",
        "while ININ: preposition or conjunction, subordinating\n",
        "    astride among uppon whether out inside pro despite on by throughout\n",
        "    below within for towards near behind atop around if like until below\n",
        "    next into if beside ...\n",
        " None\n",
        "we PRPPRP: pronoun, personal\n",
        "    hers herself him himself hisself it itself me myself one oneself ours\n",
        "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
        " None\n",
        "chase VBPVBP: verb, present tense, not 3rd person singular\n",
        "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
        "    appear tend stray glisten obtain comprise detest tease attract\n",
        "    emphasize mold postpone sever return wag ...\n",
        " None\n",
        "after ININ: preposition or conjunction, subordinating\n",
        "    astride among uppon whether out inside pro despite on by throughout\n",
        "    below within for towards near behind atop around if like until below\n",
        "    next into if beside ...\n",
        " None\n",
        "the DTDT: determiner\n",
        "    all an another any both del each either every half la many much nary\n",
        "    neither no some such that the them these this those\n",
        " None\n",
        "wind NNNN: noun, common, singular or mass\n",
        "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
        "    investment slide humour falloff slick wind hyena override subhumanity\n",
        "    machinist ...\n",
        " None\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Review the mappings in 5.4. Discuss any other examples of mappings you can think of. What type of information do they map from and to?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* word => frequency ; It maps the word to its frequency. This can be used in document classification. For example a document about fish may have the word 'fish' used often than any other document."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Using the Python interpreter in interactive mode, experiment with the dictionary examples in this chapter. Create a dictionary d, and add some entries. What happens if you try to access a non-existent entry, e.g. d['xyz']?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = dict()\n",
      "#inserts\n",
      "d['name'] = 'Nirmal kumar Ravi'\n",
      "d['job'] = 'Software Developer'\n",
      "#read\n",
      "print 'My name is %s and I work as %s'%(d['name'],d['job'])\n",
      "#access field \n",
      "if 'intrestedin' not in d:\n",
      "    print \"We will get a key error if we try d['intrestedin']\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "My name is Nirmal kumar Ravi and I work as Software Developer\n",
        "We will get a key error if we try d['intrestedin']\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Try deleting an element from a dictionary d, using the syntax del d['abc']. Check that the item was deleted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del d['job'] \n",
      "if 'job' not in d:\n",
      "    print 'deleted job'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "deleted job\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Create two dictionaries, d1 and d2, and add some entries to each. Now issue the command d1.update(d2). What did this do? What might it be useful for?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d1 = {'wow': 3, 'cat':2}\n",
      "d2 = {'dog':4, 'cat':4,'rat':2}\n",
      "\n",
      "d1.update(d2)\n",
      "print d1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'wow': 3, 'dog': 4, 'rat': 2, 'cat': 4}\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* the keys in d1 are updated by values in d2\n",
      "* can be used for combine operation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Create a dictionary e, to represent a single lexical entry for some word of your choice. Define keys like headword, part-of-speech, sense, and example, and assign them suitable values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "e = {'word':'elephant','pos':'Noun','example':'Elephants never forget'}\n",
      "for k in e:\n",
      "    print k,e[k]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "word elephant\n",
        "pos Noun\n",
        "example Elephants never forget\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Satisfy yourself that there are restrictions on the distribution of go and went, in the sense that they cannot be freely interchanged in the kinds of contexts illustrated in (3d) in 5.7."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* I go to scholl everyday\n",
      "* I went to coffe shop yesterday\n",
      "* From above two sentences we can note that go and went cannot be freely interchanged. For example 'I go to coffe shop yesterday' Does not make any sence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Train a unigram tagger and run it on some new text. Observe that some words are not assigned a tag. Why not?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
      "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
      "sent = \"IN the movie \u201cThe Big Short,\u201d Steve Carell plays a slightly altered version of me. In real life, I am a portfolio manager and financial services analyst who over a 25-year career has, at times, been highly critical of bank behavior\"\n",
      "word_tags = unigram_tagger.tag(nltk.word_tokenize(sent))\n",
      "\n",
      "[w for w,t in word_tags if t == None]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "['IN',\n",
        " '\\xe2\\x80\\x9cThe',\n",
        " 'Short',\n",
        " '\\xe2\\x80\\x9d',\n",
        " 'Carell',\n",
        " 'portfolio',\n",
        " 'analyst',\n",
        " '25-year']"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* These words or not tagged because they are not in our training set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Learn about the affix tagger (type help(nltk.AffixTagger)). Train an affix tagger and run it on some new text. Experiment with different settings for the affix length and the minimum word length. Discuss your findings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "affix_tagger = nltk.AffixTagger(train=brown_tagged_sents, affix_length=1, min_stem_length=3)\n",
      "affix_tags = unigram_tagger.tag(nltk.word_tokenize(sent))\n",
      "print affix_tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('IN', None), ('the', u'AT'), ('movie', u'NN'), ('\\xe2\\x80\\x9cThe', None), ('Big', u'JJ-TL'), ('Short', None), (',', u','), ('\\xe2\\x80\\x9d', None), ('Steve', u'NP'), ('Carell', None), ('plays', u'VBZ'), ('a', u'AT'), ('slightly', u'RB'), ('altered', u'VBN'), ('version', u'NN'), ('of', u'IN'), ('me', u'PPO'), ('.', u'.'), ('In', u'IN'), ('real', u'JJ'), ('life', u'NN'), (',', u','), ('I', u'PPSS'), ('am', u'BEM'), ('a', u'AT'), ('portfolio', None), ('manager', u'NN'), ('and', u'CC'), ('financial', u'JJ'), ('services', u'NNS'), ('analyst', None), ('who', u'WPS'), ('over', u'IN'), ('a', u'AT'), ('25-year', None), ('career', u'NN'), ('has', u'HVZ'), (',', u','), ('at', u'IN'), ('times', u'NNS'), (',', u','), ('been', u'BEN'), ('highly', u'QL'), ('critical', u'JJ'), ('of', u'IN'), ('bank', u'NN'), ('behavior', u'NN')]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* constucted by leading or trailing substring of the words\n",
      "* trained using tagged sentences\n",
      "* Some of them are not tagged because they are not found in train set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Train a bigram tagger with no backoff tagger, and run it on some of the training data. Next, run it on some new data. What happens to the performance of the tagger? Why?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown_news_tagged = brown.tagged_words(categories='news')\n",
      "size = int(len(brown_tagged_sents) * 0.9)\n",
      "train_sents = brown_tagged_sents[:size]\n",
      "test_sents = brown_tagged_sents[size:]\n",
      "unigram_tagger = nltk.BigramTagger(train_sents)\n",
      "print '%f accuracy on train'%(unigram_tagger.evaluate(train_sents))\n",
      "print '%f accuracy on test'%(unigram_tagger.evaluate(test_sents))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.786436 accuracy on train\n",
        "0.102761 accuracy on test"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The perfomance of tagger on train is higher than perfomance on test. \n",
      "* This is obivous because as we trained our model on train_sents It does well than unseen data which is test_sents"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> We can use a dictionary to specify the values to be substituted into a formatting string. Read Python's library documentation for formatting strings http://docs.python.org/lib/typesseq-strings.html and use this method to display today's date in two different formats."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Day {0} Month {1} Year {2}'.format('2', '7', '2016')\n",
      "print '{0}/{1}/{2}'.format('2', '7', '2016')\n",
      "print '{1}/{0}/{2}'.format('2', '7', '2016')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Day 2 Month 7 Year 2016\n",
        "2/7/2016\n",
        "7/2/2016\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Use sorted() and set() to get a sorted list of tags used in the Brown corpus, removing duplicates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sorted(set(brown.tagged_words()))[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'!', u'.'), (u'!', u'.-HL'), (u'$.027', u'NNS'), (u'$.03', u'NNS'), (u'$.03', u'NNS-HL'), (u'$.054/mbf', u'NNS'), (u'$.07', u'NNS'), (u'$.07/cwt', u'NNS'), (u'$.076', u'NNS'), (u'$.09', u'NNS'), (u'$.10-a-minute', u'NN-HL'), (u'$.105', u'NNS'), (u'$.12', u'NNS'), (u'$.30', u'NNS'), (u'$.30/mbf', u'NNS'), (u'$.50', u'NN'), (u'$.50', u'NNS'), (u'$.65', u'NNS'), (u'$.75', u'NNS'), (u'$.80', u'NNS')]\n"
       ]
      }
     ],
     "prompt_number": 35
    }
   ],
   "metadata": {}
  }
 ]
}